{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282a088-298f-4d8b-ba24-5789aa440bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nnunetv2\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"nnU-Net v2 imported successfully!\")\n",
    "\n",
    "# Test that you can access nnunetv2 modules\n",
    "print(\"nnunetv2 location:\", nnunetv2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaeca7-0cf7-4026-9ccd-24e5e552a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nnunetv2, torch\n",
    "\n",
    "# Work directories - updated for your local setup\n",
    "BASE = \"D:/nnunet_with_classification/data\"\n",
    "RAW  = f\"{BASE}/nnUNet_raw\"\n",
    "PREP = f\"{BASE}/nnUNet_preprocessed\"\n",
    "RES  = f\"{BASE}/nnUNet_results\"\n",
    "\n",
    "# Create nnU-Net directories\n",
    "os.makedirs(RAW, exist_ok=True)\n",
    "os.makedirs(PREP, exist_ok=True)\n",
    "os.makedirs(RES, exist_ok=True)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"nnUNet_raw\"] = RAW\n",
    "os.environ[\"nnUNet_preprocessed\"] = PREP\n",
    "os.environ[\"nnUNet_results\"] = RES\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "print(\"nnU-Net v2: imported successfully!\")\n",
    "print(f\"nnU-Net directories created at: {BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3405a90-353b-41cf-99f6-4659c37083b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagesTr: 288\n",
      "labelsTr: 288\n",
      "imagesTs: 72\n",
      "Mapped classification labels: 288\n",
      "Wrote dataset.json, splits_final.json, classification_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pathlib, shutil, re, json, csv\n",
    "\n",
    "# >>>>>>>>>>>> UPDATE THIS PATH <<<<<<<<<<<<\n",
    "SRC = \"D:/nnunet_with_classification/data\"\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "RAW  = os.environ[\"nnUNet_raw\"]\n",
    "PREP = os.environ[\"nnUNet_preprocessed\"]\n",
    "DSROOT = f\"{RAW}/{DSNAME}\"\n",
    "imgTr = f\"{DSROOT}/imagesTr\"; lblTr = f\"{DSROOT}/labelsTr\"; imgTs = f\"{DSROOT}/imagesTs\"\n",
    "for d in (imgTr, lblTr, imgTs): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def stem(p): return pathlib.Path(p).name.replace(\".nii.gz\",\"\")\n",
    "sub_regex = re.compile(r\"subtype\\s*([012])\", re.IGNORECASE)\n",
    "\n",
    "def find_split(name):\n",
    "    # case-insensitive split folder lookup\n",
    "    for cand in os.listdir(SRC):\n",
    "        if cand.lower() == name:\n",
    "            p = os.path.join(SRC, cand)\n",
    "            if os.path.isdir(p): return p\n",
    "    return None\n",
    "\n",
    "train_dir = find_split(\"train\")\n",
    "val_dir   = find_split(\"validation\")\n",
    "test_dir  = find_split(\"test\")\n",
    "assert train_dir and val_dir and test_dir, f\"Could not find train/validation/test under {SRC}\"\n",
    "\n",
    "def ingest_split(split_dir, cls_map):\n",
    "    imgs = glob.glob(os.path.join(split_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)\n",
    "    used = 0\n",
    "    for img in sorted(imgs):\n",
    "        case = stem(img).replace(\"_0000\",\"\")\n",
    "        msk  = img.replace(\"_0000.nii.gz\",\".nii.gz\")\n",
    "        if not os.path.exists(msk):\n",
    "            continue\n",
    "        shutil.copy(img, f\"{imgTr}/{case}_0000.nii.gz\")\n",
    "        shutil.copy(msk, f\"{lblTr}/{case}.nii.gz\")\n",
    "        used += 1\n",
    "        # infer subtype from folder names (expects 'subtype0/1/2')\n",
    "        sub_idx = None\n",
    "        for part in pathlib.Path(img).parts:\n",
    "            m = sub_regex.search(part)\n",
    "            if m:\n",
    "                sub_idx = int(m.group(1)); break\n",
    "        if sub_idx is not None:\n",
    "            cls_map[case] = sub_idx\n",
    "    return used\n",
    "\n",
    "cls_map = {}\n",
    "n_tr = ingest_split(train_dir, cls_map)\n",
    "n_va = ingest_split(val_dir,   cls_map)\n",
    "\n",
    "# test images\n",
    "for img in sorted(glob.glob(os.path.join(test_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)):\n",
    "    case = stem(img).replace(\"_0000\",\"\")\n",
    "    shutil.copy(img, f\"{imgTs}/{case}_0000.nii.gz\")\n",
    "\n",
    "print(\"imagesTr:\", len(glob.glob(f\"{imgTr}/*_0000.nii.gz\")))\n",
    "print(\"labelsTr:\", len(glob.glob(f\"{lblTr}/*.nii.gz\")))\n",
    "print(\"imagesTs:\", len(glob.glob(f\"{imgTs}/*_0000.nii.gz\")))\n",
    "print(\"Mapped classification labels:\", len(cls_map))\n",
    "\n",
    "# dataset.json\n",
    "dataset_json = {\n",
    "  \"name\": DSNAME,\n",
    "  \"tensorImageSize\": \"3D\",\n",
    "  \"modality\": {\"0\": \"CT\"},\n",
    "  \"labels\": {\"background\": 0, \"pancreas\": 1, \"lesion\": 2},\n",
    "  \"numTraining\": len(glob.glob(f\"{lblTr}/*.nii.gz\")),\n",
    "  \"numTest\": len(glob.glob(f\"{imgTs}/*_0000.nii.gz\")),\n",
    "  \"training\": [{\"image\": f\"./imagesTr/{stem(i)}.nii.gz\",\n",
    "                \"label\": f\"./labelsTr/{stem(i).replace('_0000','')}.nii.gz\"}\n",
    "               for i in sorted(glob.glob(f\"{imgTr}/*_0000.nii.gz\"))],\n",
    "  \"test\": [f\"./imagesTs/{stem(i)}.nii.gz\" for i in sorted(glob.glob(f\"{imgTs}/*_0000.nii.gz\"))]\n",
    "}\n",
    "os.makedirs(DSROOT, exist_ok=True)\n",
    "with open(f\"{DSROOT}/dataset.json\",\"w\") as f: json.dump(dataset_json, f, indent=2)\n",
    "\n",
    "# splits_final.json (use original validation as nnU-Net val)\n",
    "val_cases = {stem(p).replace(\"_0000\",\"\") for p in glob.glob(os.path.join(val_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)}\n",
    "all_cases = sorted([stem(p).replace(\"_0000\",\"\") for p in glob.glob(f\"{imgTr}/*_0000.nii.gz\")])\n",
    "train_cases = [c for c in all_cases if c not in val_cases]\n",
    "spdir = f\"{PREP}/{DSNAME}\"\n",
    "os.makedirs(spdir, exist_ok=True)\n",
    "with open(f\"{spdir}/splits_final.json\",\"w\") as f:\n",
    "    json.dump([{\"train\": train_cases, \"val\": sorted(list(val_cases))}], f, indent=2)\n",
    "\n",
    "# classification_labels.csv\n",
    "csv_path = f\"{spdir}/classification_labels.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    for c in all_cases:\n",
    "        if c in cls_map:\n",
    "            w.writerow([c, cls_map[c]])\n",
    "        else:\n",
    "            # if you see many misses here, your folders may not be named 'subtype0/1/2'\n",
    "            pass\n",
    "\n",
    "print(\"Wrote dataset.json, splits_final.json, classification_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1501568-3b84-4289-9517-7ac312cc9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote v2 dataset.json at: D:/nnunet_with_classification/data/nnUNet_raw/Dataset777_M31Quiz/dataset.json\n",
      "numTraining: 288\n"
     ]
    }
   ],
   "source": [
    "import json, glob, os, pathlib\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "RAW   = os.environ[\"nnUNet_raw\"]\n",
    "DSROOT = f\"{RAW}/{DSNAME}\"\n",
    "lblTr = f\"{DSROOT}/labelsTr\"\n",
    "\n",
    "dataset_v2 = {\n",
    "    \"channel_names\": { \"0\": \"CT\" },               # required in v2\n",
    "    \"labels\": { \"background\": 0, \"pancreas\": 1, \"lesion\": 2 },  # your classes\n",
    "    \"numTraining\": len(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "with open(f\"{DSROOT}/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset_v2, f, indent=2)\n",
    "\n",
    "print(\"✅ Wrote v2 dataset.json at:\", f\"{DSROOT}/dataset.json\")\n",
    "print(\"numTraining:\", dataset_v2[\"numTraining\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120217f3-ddd3-417f-b7fd-67c08ebebb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks needing fix: 214\n",
      "   quiz_0_060.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_066.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_077.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_117.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_126.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_139.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_145.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_150.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_159.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "   quiz_0_160.nii.gz unique: [0.         1.00001526 2.        ]\n",
      "Remaining masks with non-{0,1,2} labels: 0\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib, numpy as np, glob, os, pathlib\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "lblTr = os.path.join(os.environ[\"nnUNet_raw\"], DSNAME, \"labelsTr\")\n",
    "\n",
    "bad = []\n",
    "for p in sorted(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))):\n",
    "    img = nib.load(p)\n",
    "    arr = img.get_fdata()  # floats possible\n",
    "    uniq = np.unique(arr)\n",
    "    if not np.all(np.isin(uniq, [0,1,2])):\n",
    "        bad.append((p, uniq))\n",
    "\n",
    "print(\"Masks needing fix:\", len(bad))\n",
    "for p, uniq in bad[:10]:\n",
    "    print(\"  \", pathlib.Path(p).name, \"unique:\", uniq[:10])\n",
    "\n",
    "# Fix: round and clip to {0,1,2}, write as uint8\n",
    "for p, uniq in bad:\n",
    "    img = nib.load(p)\n",
    "    arr = img.get_fdata()\n",
    "    arr = np.rint(arr)           # round to nearest integer\n",
    "    arr = np.clip(arr, 0, 2)     # enforce label set\n",
    "    arr = arr.astype(np.uint8)\n",
    "\n",
    "    hdr = img.header.copy()\n",
    "    hdr.set_data_dtype(np.uint8)\n",
    "    # avoid unintended scaling\n",
    "    hdr[\"scl_slope\"] = 1\n",
    "    hdr[\"scl_inter\"] = 0\n",
    "\n",
    "    fixed = nib.Nifti1Image(arr, img.affine, hdr)\n",
    "    nib.save(fixed, p)\n",
    "\n",
    "# Sanity check after fix\n",
    "remaining = []\n",
    "for p in sorted(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))):\n",
    "    arr = nib.load(p).get_fdata()\n",
    "    uniq = np.unique(arr)\n",
    "    if not np.all(np.isin(uniq, [0,1,2])):\n",
    "        remaining.append((p, uniq))\n",
    "print(\"Remaining masks with non-{0,1,2} labels:\", len(remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b5c41-bdc0-4955-9ec2-026fd80a8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNetv2_plan_and_preprocess -d 777 -c 3d_fullres --verify_dataset_integrity -pl nnUNetPlannerResEncM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6409a5b-3511-42e0-8ac7-7f9dff1a2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNetv2_train 777 3d_fullres 0 -tr NNUNet -p nnUNetResEncUNetMPlans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c73bcb-1d70-46bd-bb39-a9dfa1140758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making folder flat, not having sub folders as nnUnetv2 expects a flat folder not subfolds\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Create a flat validation folder for nnU-Net\n",
    "val_flat_dir = \"D:/nnunet_with_classification/data/validation_flat\"\n",
    "os.makedirs(val_flat_dir, exist_ok=True)\n",
    "\n",
    "# Copy all validation images to flat folder\n",
    "val_source = \"D:/nnunet_implementation_m31_assessment/data/validation\"\n",
    "val_files = glob.glob(os.path.join(val_source, \"**\", \"*_0000.nii.gz\"), recursive=True)\n",
    "\n",
    "print(f\"Copying {len(val_files)} validation files to flat folder...\")\n",
    "for src_file in val_files:\n",
    "    filename = os.path.basename(src_file)\n",
    "    dst_file = os.path.join(val_flat_dir, filename)\n",
    "    shutil.copy2(src_file, dst_file)\n",
    "    print(f\"  Copied: {filename}\")\n",
    "\n",
    "print(f\"✅ Flat validation folder ready: {val_flat_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2de06-e281-442d-a3d1-73c89e74d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNetv2_train 777 3d_fullres 0 -tr NNUNet_tuned -p nnUNetResEncUNetMPlans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89484518-9274-4acb-87fa-9a26bc376213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference for validation data for segementation\n",
    "# After creating flat folder:\n",
    "!nnUNetv2_predict \\\n",
    "  -i D:/nnunet_with_classification/data/test \\\n",
    "  -o D:/nnunet_with_classification/data/predictions_validation \\\n",
    "  -d 777 \\\n",
    "  -f 0 \\\n",
    "  -tr NNUNet_tuned  \\\n",
    "  -c 3d_fullres \\\n",
    "  -p nnUNetResEncUNetMPlans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaaed0-518b-47ed-a025-9685254b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Validation Classification Inference (FIXED for FP16/FP32 mismatch)\n",
    "\n",
    "import os, json, torch, csv, nibabel as nib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiScaleClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale feature fusion classification head with attention mechanism\n",
    "    (Must match exactly what's in your NNUNet.py)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels: List[int], num_classes: int, dim: int = 3, \n",
    "                 target_channels: int = 256, spatial_reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.num_scales = 3  # Use last 3 encoder stages\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Multi-scale feature adapters\n",
    "        self.feature_adapters = nn.ModuleList()\n",
    "        \n",
    "        for channels in encoder_channels[-self.num_scales:]:\n",
    "            if dim == 3:\n",
    "                adapter = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool3d((spatial_reduction, spatial_reduction, spatial_reduction)),\n",
    "                    nn.Conv3d(channels, target_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm3d(target_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout3d(0.1)\n",
    "                )\n",
    "            else:\n",
    "                adapter = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((spatial_reduction, spatial_reduction)),\n",
    "                    nn.Conv2d(channels, target_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(target_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout2d(0.1)\n",
    "                )\n",
    "            self.feature_adapters.append(adapter)\n",
    "        \n",
    "        # Global pooling for each scale\n",
    "        if dim == 3:\n",
    "            self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        else:\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Attention mechanism to weight different scales\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(target_channels * self.num_scales, target_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(target_channels, self.num_scales),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(target_channels, target_channels),\n",
    "            nn.BatchNorm1d(target_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(target_channels, target_channels // 2),\n",
    "            nn.BatchNorm1d(target_channels // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(target_channels // 2, target_channels // 4),\n",
    "            nn.BatchNorm1d(target_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(target_channels // 4, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_features: List of feature tensors from encoder stages\n",
    "        Returns:\n",
    "            Classification logits [B, num_classes]\n",
    "        \"\"\"\n",
    "        if len(encoder_features) < self.num_scales:\n",
    "            raise ValueError(f\"Expected at least {self.num_scales} encoder features, \"\n",
    "                           f\"got {len(encoder_features)}\")\n",
    "        \n",
    "        # Process multi-scale features\n",
    "        multi_scale_features = []\n",
    "        \n",
    "        for feat, adapter in zip(encoder_features[-self.num_scales:], self.feature_adapters):\n",
    "            # Adapt features to common channel size and spatial resolution\n",
    "            adapted = adapter(feat)\n",
    "            # Global pooling to get feature vector\n",
    "            pooled = self.global_pool(adapted).flatten(1)\n",
    "            multi_scale_features.append(pooled)\n",
    "        \n",
    "        # Stack all scale features\n",
    "        stacked_features = torch.stack(multi_scale_features, dim=1)  # [B, num_scales, target_channels]\n",
    "        \n",
    "        # Concatenate for attention computation\n",
    "        concat_features = torch.cat(multi_scale_features, dim=1)  # [B, num_scales * target_channels]\n",
    "        \n",
    "        # Compute attention weights for different scales\n",
    "        attention_weights = self.attention(concat_features)  # [B, num_scales]\n",
    "        \n",
    "        # Apply attention weights to aggregate multi-scale features\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # [B, num_scales, 1]\n",
    "        weighted_features = (stacked_features * attention_weights).sum(dim=1)  # [B, target_channels]\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused_features = self.feature_fusion(weighted_features)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ---- PATHS ----\n",
    "MODEL_DIR = Path(\"D:/nnunet_with_classification/data/nnUNet_results/Dataset777_M31Quiz/NNUNet_tuned__nnUNetResEncUNetMPlans__3d_fullres\")\n",
    "FOLD_DIR  = MODEL_DIR / \"fold_0\"\n",
    "CKPT      = FOLD_DIR / \"checkpoint_final.pth\"\n",
    "\n",
    "IMAGES_TS = Path(\"D:/nnunet_with_classification/data/test\")\n",
    "OUT_DIR   = Path(\"D:/nnunet_with_classification/predictions_validation\")\n",
    "CSV_OUT   = OUT_DIR / \"subtype_results.csv\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Build Predictor ----\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=True,\n",
    ")\n",
    "\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    model_training_output_dir=str(MODEL_DIR),\n",
    "    use_folds=[0],\n",
    "    checkpoint_name=CKPT.name,\n",
    ")\n",
    "\n",
    "net = predictor.network\n",
    "\n",
    "# ---- Load Classification Head ----\n",
    "print(\"🔍 Loading checkpoint...\")\n",
    "ckpt = torch.load(CKPT, map_location=device, weights_only=False)\n",
    "cls_sd = ckpt.get(\"cls_state_dict\", None)\n",
    "\n",
    "encoder_channels = net.encoder.output_channels\n",
    "print(f\"Encoder channels: {encoder_channels}\")\n",
    "\n",
    "classifier = MultiScaleClassificationHead(\n",
    "    encoder_channels=encoder_channels,\n",
    "    num_classes=3,\n",
    "    dim=3,\n",
    "    target_channels=256,\n",
    "    spatial_reduction=4\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(cls_sd, strict=True)\n",
    "print(\"✅ Successfully loaded MultiScaleClassificationHead weights\")\n",
    "\n",
    "# ---- Setup Feature Capture ----\n",
    "encoder_features = []\n",
    "\n",
    "def create_hook(stage_idx):\n",
    "    def hook_fn(module, input, output):\n",
    "        while len(encoder_features) <= stage_idx:\n",
    "            encoder_features.append(None)\n",
    "        encoder_features[stage_idx] = output\n",
    "    return hook_fn\n",
    "\n",
    "hooks = []\n",
    "for i, stage in enumerate(net.encoder.stages):\n",
    "    hook = stage.register_forward_hook(create_hook(i))\n",
    "    hooks.append(hook)\n",
    "\n",
    "net.eval()\n",
    "classifier.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ---- Find Validation Images ----\n",
    "case_files = []\n",
    "for subfolder in [\"subtype0\", \"subtype1\", \"subtype2\"]:\n",
    "    subfolder_path = IMAGES_TS / subfolder\n",
    "    if subfolder_path.exists():\n",
    "        files = list(subfolder_path.glob(\"*_0000.nii.gz\"))\n",
    "        case_files.extend(files)\n",
    "\n",
    "case_files = sorted(case_files)\n",
    "print(f\"\\n🔍 Found {len(case_files)} validation files\")\n",
    "\n",
    "# ---- Process Cases ----\n",
    "rows = [(\"Names\", \"Subtype\")]\n",
    "\n",
    "for img in case_files:\n",
    "    case_id = img.name.replace(\"_0000.nii.gz\", \"\") + \".nii.gz\"\n",
    "    print(f\"Processing: {img.name}\")\n",
    "\n",
    "    encoder_features.clear()\n",
    "    temp_output = OUT_DIR / f\"temp_{case_id}\"\n",
    "    \n",
    "    try:\n",
    "        predictor.predict_from_files(\n",
    "            [[str(img)]], \n",
    "            [str(temp_output)],\n",
    "            save_probabilities=False,\n",
    "            overwrite=True,\n",
    "            num_processes_preprocessing=1,\n",
    "            num_processes_segmentation_export=1\n",
    "        )\n",
    "        \n",
    "        if temp_output.exists():\n",
    "            temp_output.unlink()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {img.name}: {e}\")\n",
    "        pred = 0\n",
    "        rows.append((case_id, pred))\n",
    "        continue\n",
    "\n",
    "    # Classification with FP16/FP32 fix\n",
    "    if len(encoder_features) >= 3:\n",
    "        try:\n",
    "            valid_features = [f for f in encoder_features if f is not None]\n",
    "            \n",
    "            if len(valid_features) >= 3:\n",
    "                # 🔧 FIX: Convert features to float32 to match classifier weights\n",
    "                valid_features_float = [f.float() for f in valid_features]\n",
    "                logits = classifier(valid_features_float)\n",
    "                pred = int(torch.argmax(logits, dim=1).item())\n",
    "            else:\n",
    "                print(f\"⚠️  Insufficient features captured for {img.name}\")\n",
    "                pred = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Classification error for {img.name}: {e}\")\n",
    "            pred = 0\n",
    "    else:\n",
    "        print(f\"⚠️  No encoder features captured for {img.name}\")\n",
    "        pred = 0\n",
    "\n",
    "    rows.append((case_id, pred))\n",
    "    print(f\"  ✅ Classification: {pred}\")\n",
    "\n",
    "# Cleanup\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# ---- Save Results ----\n",
    "with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(f\"\\n✅ Validation Classification CSV written: {CSV_OUT}\")\n",
    "\n",
    "# Show distribution\n",
    "import pandas as pd\n",
    "df = pd.read_csv(CSV_OUT)\n",
    "print(f\"\\nValidation Classification Results:\")\n",
    "print(f\"   Total cases: {len(df)}\")\n",
    "print(f\"   Distribution:\")\n",
    "print(df['Subtype'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9532f-92c2-4f25-829f-55cd9691f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation script for your results on validation data\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Validation cases\n",
    "validation_cases = [\n",
    "    \"quiz_0_168\", \"quiz_0_171\", \"quiz_0_174\", \"quiz_0_184\", \"quiz_0_187\", \n",
    "    \"quiz_0_189\", \"quiz_0_244\", \"quiz_0_253\", \"quiz_0_254\", \"quiz_1_090\",\n",
    "    \"quiz_1_093\", \"quiz_1_094\", \"quiz_1_154\", \"quiz_1_158\", \"quiz_1_164\",\n",
    "    \"quiz_1_166\", \"quiz_1_211\", \"quiz_1_213\", \"quiz_1_221\", \"quiz_1_227\",\n",
    "    \"quiz_1_231\", \"quiz_1_242\", \"quiz_1_331\", \"quiz_1_335\", \"quiz_2_074\",\n",
    "    \"quiz_2_080\", \"quiz_2_084\", \"quiz_2_085\", \"quiz_2_088\", \"quiz_2_089\",\n",
    "    \"quiz_2_098\", \"quiz_2_191\", \"quiz_2_241\", \"quiz_2_364\", \"quiz_2_377\",\n",
    "    \"quiz_2_379\"\n",
    "]\n",
    "\n",
    "# Paths\n",
    "gt_seg_dir = f\"{os.environ['nnUNet_raw']}/Dataset777_M31Quiz/labelsTr\"\n",
    "pred_seg_dir = \"D:/nnunet_with_classification/data/predictions_validation\"\n",
    "gt_cls_csv = f\"{os.environ['nnUNet_preprocessed']}/Dataset777_M31Quiz/classification_labels.csv\"\n",
    "pred_cls_csv = \"D:/nnunet_with_classification/predictions_validation/subtype_results.csv\"\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    total = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2.0 * intersection) / total if total > 0 else 1.0\n",
    "\n",
    "# Check paths\n",
    "print(\"🔍 Checking paths...\")\n",
    "print(f\"GT segmentation dir: {os.path.exists(gt_seg_dir)}\")\n",
    "print(f\"Predicted segmentation dir: {os.path.exists(pred_seg_dir)}\")\n",
    "print(f\"GT classification CSV: {os.path.exists(gt_cls_csv)}\")\n",
    "print(f\"Predicted classification CSV: {os.path.exists(pred_cls_csv)}\")\n",
    "\n",
    "# Calculate segmentation metrics\n",
    "pancreas_dices = []\n",
    "lesion_dices = []\n",
    "missing_files = []\n",
    "\n",
    "print(\"\\n📊 Computing segmentation metrics...\")\n",
    "for case in validation_cases:\n",
    "    gt_file = f\"{gt_seg_dir}/{case}.nii.gz\"\n",
    "    pred_file = f\"{pred_seg_dir}/{case}.nii.gz\"\n",
    "    \n",
    "    if os.path.exists(gt_file) and os.path.exists(pred_file):\n",
    "        try:\n",
    "            gt = nib.load(gt_file).get_fdata()\n",
    "            pred = nib.load(pred_file).get_fdata()\n",
    "            \n",
    "            # Whole pancreas (label > 0)\n",
    "            gt_pancreas = (gt > 0).astype(int)\n",
    "            pred_pancreas = (pred > 0).astype(int)\n",
    "            pancreas_dice = dice_score(gt_pancreas, pred_pancreas)\n",
    "            pancreas_dices.append(pancreas_dice)\n",
    "            \n",
    "            # Lesion only (label == 2)\n",
    "            gt_lesion = (gt == 2).astype(int)\n",
    "            pred_lesion = (pred == 2).astype(int)\n",
    "            lesion_dice = dice_score(gt_lesion, pred_lesion)\n",
    "            lesion_dices.append(lesion_dice)\n",
    "            \n",
    "            print(f\"{case}: Pancreas={pancreas_dice:.3f}, Lesion={lesion_dice:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {case}: {e}\")\n",
    "            missing_files.append(case)\n",
    "    else:\n",
    "        missing_files.append(case)\n",
    "        print(f\"Missing files for {case}\")\n",
    "\n",
    "if pancreas_dices:\n",
    "    whole_pancreas_dsc = np.mean(pancreas_dices)\n",
    "    lesion_dsc = np.mean(lesion_dices)\n",
    "    \n",
    "    print(f\"\\n📈 Segmentation Statistics:\")\n",
    "    print(f\"Pancreas DSC - Mean: {whole_pancreas_dsc:.4f}, Std: {np.std(pancreas_dices):.4f}\")\n",
    "    print(f\"Lesion DSC - Mean: {lesion_dsc:.4f}, Std: {np.std(lesion_dices):.4f}\")\n",
    "else:\n",
    "    whole_pancreas_dsc = 0.0\n",
    "    lesion_dsc = 0.0\n",
    "\n",
    "# Classification metrics\n",
    "print(\"\\n🎯 Computing classification metrics...\")\n",
    "gt_labels = {}\n",
    "with open(gt_cls_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            gt_labels[row[0]] = int(row[1])\n",
    "\n",
    "pred_labels = {}\n",
    "with open(pred_cls_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            case_name = row[0].replace('.nii.gz', '')\n",
    "            pred_labels[case_name] = int(row[1])\n",
    "\n",
    "# Get validation classification results\n",
    "val_gt = []\n",
    "val_pred = []\n",
    "for case in validation_cases:\n",
    "    if case in gt_labels and case in pred_labels:\n",
    "        val_gt.append(gt_labels[case])\n",
    "        val_pred.append(pred_labels[case])\n",
    "\n",
    "if val_gt and val_pred:\n",
    "    macro_f1 = f1_score(val_gt, val_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n🔍 Classification Analysis:\")\n",
    "    print(f\"Ground truth distribution: {np.bincount(val_gt)}\")\n",
    "    print(f\"Prediction distribution: {np.bincount(val_pred)}\")\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(val_gt, val_pred, target_names=['Subtype 0', 'Subtype 1', 'Subtype 2']))\n",
    "else:\n",
    "    macro_f1 = 0.0\n",
    "\n",
    "# Final Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎓 PhD LEVEL REQUIREMENTS EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Whole Pancreas DSC: {whole_pancreas_dsc:.4f} ≥ 0.91: {'✅ PASS' if whole_pancreas_dsc >= 0.91 else 'Below expectations'}\")\n",
    "print(f\"Lesion DSC: {lesion_dsc:.4f} ≥ 0.31: {'✅ PASS' if lesion_dsc >= 0.31 else 'Below expectations'}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f} ≥ 0.70: {'✅ PASS' if macro_f1 >= 0.70 else 'Below expectations'}\")\n",
    "\n",
    "overall_pass = (whole_pancreas_dsc >= 0.91) and (lesion_dsc >= 0.31) and (macro_f1 >= 0.70)\n",
    "print(f\"\\n🎯 OVERALL RESULT: {'✅ PASS' if overall_pass else 'Below expectations '}\")\n",
    "\n",
    "# Summary for your report\n",
    "print(f\"\\n📋 SUMMARY FOR REPORT:\")\n",
    "print(f\"Processed {len(pancreas_dices)} segmentation cases and {len(val_gt)} classification cases\")\n",
    "print(f\"Whole Pancreas DSC: {whole_pancreas_dsc:.4f}\")\n",
    "print(f\"Lesion DSC: {lesion_dsc:.4f}\")\n",
    "print(f\"Classification Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"⚠️ Missing files for {len(missing_files)} cases: {missing_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70cf72-37ac-4dda-b7c2-238859c43f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference for test data\n",
    "\n",
    "!nnUNetv2_predict \\\n",
    "  -i D:/nnunet_with_classification/data/test \\\n",
    "  -o D:/nnunet_with_classification/data/predictions_test_label \\\n",
    "  -d 777 \\\n",
    "  -f 0 \\\n",
    "  -tr NNUNet_tuned  \\\n",
    "  -c 3d_fullres \\\n",
    "  -p nnUNetResEncUNetMPlans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455f8e5-f41e-4487-a16f-3de710ba6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Test labels Inference for classification (FIXED for FP16/FP32 mismatch + File Discovery)\n",
    "# These wont be correct as the model has not learned well on training data. \n",
    "import os, json, torch, csv, nibabel as nib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiScaleClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale feature fusion classification head with attention mechanism\n",
    "    (Must match exactly what's in your NNUNet.py)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels: List[int], num_classes: int, dim: int = 3, \n",
    "                 target_channels: int = 256, spatial_reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.num_scales = 3  # Use last 3 encoder stages\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Multi-scale feature adapters\n",
    "        self.feature_adapters = nn.ModuleList()\n",
    "        \n",
    "        for channels in encoder_channels[-self.num_scales:]:\n",
    "            if dim == 3:\n",
    "                adapter = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool3d((spatial_reduction, spatial_reduction, spatial_reduction)),\n",
    "                    nn.Conv3d(channels, target_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm3d(target_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout3d(0.1)\n",
    "                )\n",
    "            else:\n",
    "                adapter = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((spatial_reduction, spatial_reduction)),\n",
    "                    nn.Conv2d(channels, target_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(target_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout2d(0.1)\n",
    "                )\n",
    "            self.feature_adapters.append(adapter)\n",
    "        \n",
    "        # Global pooling for each scale\n",
    "        if dim == 3:\n",
    "            self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        else:\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Attention mechanism to weight different scales\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(target_channels * self.num_scales, target_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(target_channels, self.num_scales),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(target_channels, target_channels),\n",
    "            nn.BatchNorm1d(target_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(target_channels, target_channels // 2),\n",
    "            nn.BatchNorm1d(target_channels // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(target_channels // 2, target_channels // 4),\n",
    "            nn.BatchNorm1d(target_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(target_channels // 4, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_features: List of feature tensors from encoder stages\n",
    "        Returns:\n",
    "            Classification logits [B, num_classes]\n",
    "        \"\"\"\n",
    "        if len(encoder_features) < self.num_scales:\n",
    "            raise ValueError(f\"Expected at least {self.num_scales} encoder features, \"\n",
    "                           f\"got {len(encoder_features)}\")\n",
    "        \n",
    "        # Process multi-scale features\n",
    "        multi_scale_features = []\n",
    "        \n",
    "        for feat, adapter in zip(encoder_features[-self.num_scales:], self.feature_adapters):\n",
    "            # Adapt features to common channel size and spatial resolution\n",
    "            adapted = adapter(feat)\n",
    "            # Global pooling to get feature vector\n",
    "            pooled = self.global_pool(adapted).flatten(1)\n",
    "            multi_scale_features.append(pooled)\n",
    "        \n",
    "        # Stack all scale features\n",
    "        stacked_features = torch.stack(multi_scale_features, dim=1)  # [B, num_scales, target_channels]\n",
    "        \n",
    "        # Concatenate for attention computation\n",
    "        concat_features = torch.cat(multi_scale_features, dim=1)  # [B, num_scales * target_channels]\n",
    "        \n",
    "        # Compute attention weights for different scales\n",
    "        attention_weights = self.attention(concat_features)  # [B, num_scales]\n",
    "        \n",
    "        # Apply attention weights to aggregate multi-scale features\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # [B, num_scales, 1]\n",
    "        weighted_features = (stacked_features * attention_weights).sum(dim=1)  # [B, target_channels]\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused_features = self.feature_fusion(weighted_features)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ---- PATHS ----\n",
    "MODEL_DIR = Path(\"D:/nnunet_with_classification/data/nnUNet_results/Dataset777_M31Quiz/NNUNet_tuned__nnUNetResEncUNetMPlans__3d_fullres\")\n",
    "FOLD_DIR  = MODEL_DIR / \"fold_0\"\n",
    "CKPT      = FOLD_DIR / \"checkpoint_final.pth\"\n",
    "\n",
    "IMAGES_TS = Path(\"D:/nnunet_with_classification/data/test\")\n",
    "OUT_DIR   = Path(\"D:/nnunet_with_classification/data/predictions_test_label\")\n",
    "CSV_OUT   = OUT_DIR / \"subtype_results.csv\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Build Predictor ----\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=True,\n",
    ")\n",
    "\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    model_training_output_dir=str(MODEL_DIR),\n",
    "    use_folds=[0],\n",
    "    checkpoint_name=CKPT.name,\n",
    ")\n",
    "\n",
    "net = predictor.network\n",
    "\n",
    "# ---- Load Classification Head ----\n",
    "print(\"🔍 Loading checkpoint...\")\n",
    "ckpt = torch.load(CKPT, map_location=device, weights_only=False)\n",
    "cls_sd = ckpt.get(\"cls_state_dict\", None)\n",
    "\n",
    "encoder_channels = net.encoder.output_channels\n",
    "print(f\"Encoder channels: {encoder_channels}\")\n",
    "\n",
    "classifier = MultiScaleClassificationHead(\n",
    "    encoder_channels=encoder_channels,\n",
    "    num_classes=3,\n",
    "    dim=3,\n",
    "    target_channels=256,\n",
    "    spatial_reduction=4\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(cls_sd, strict=True)\n",
    "print(\"✅ Successfully loaded MultiScaleClassificationHead weights\")\n",
    "\n",
    "# ---- Setup Feature Capture ----\n",
    "encoder_features = []\n",
    "\n",
    "def create_hook(stage_idx):\n",
    "    def hook_fn(module, input, output):\n",
    "        while len(encoder_features) <= stage_idx:\n",
    "            encoder_features.append(None)\n",
    "        encoder_features[stage_idx] = output\n",
    "    return hook_fn\n",
    "\n",
    "hooks = []\n",
    "for i, stage in enumerate(net.encoder.stages):\n",
    "    hook = stage.register_forward_hook(create_hook(i))\n",
    "    hooks.append(hook)\n",
    "\n",
    "net.eval()\n",
    "classifier.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ---- Find Test Images ----\n",
    "print(f\"🔍 Looking for test files in: {IMAGES_TS}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not IMAGES_TS.exists():\n",
    "    print(f\"❌ Test directory does not exist: {IMAGES_TS}\")\n",
    "    exit(1)\n",
    "\n",
    "# Look for files directly in the test directory (flat structure)\n",
    "case_files = list(IMAGES_TS.glob(\"*_0000.nii.gz\"))\n",
    "case_files = sorted(case_files)\n",
    "\n",
    "print(f\"🔍 Found {len(case_files)} test files\")\n",
    "\n",
    "# Debug: Show first few files found\n",
    "if len(case_files) > 0:\n",
    "    print(\"📋 Sample files found:\")\n",
    "    for i, file in enumerate(case_files[:5]):  # Show first 5 files\n",
    "        print(f\"   {i+1}. {file.name}\")\n",
    "    if len(case_files) > 5:\n",
    "        print(f\"   ... and {len(case_files) - 5} more files\")\n",
    "else:\n",
    "    print(\"❌ No files matching pattern '*_0000.nii.gz' found!\")\n",
    "    print(\"📋 Files in directory:\")\n",
    "    all_files = list(IMAGES_TS.glob(\"*\"))\n",
    "    for file in all_files[:10]:  # Show first 10 files\n",
    "        print(f\"   {file.name}\")\n",
    "    if len(all_files) > 10:\n",
    "        print(f\"   ... and {len(all_files) - 10} more files\")\n",
    "    exit(1)\n",
    "\n",
    "# ---- Process Cases ----\n",
    "rows = [(\"Names\", \"Subtype\")]\n",
    "\n",
    "for img in case_files:\n",
    "    # For files like 'quiz_037_0000.nii.gz', we want 'quiz_037.nii.gz'\n",
    "    case_id = img.name.replace(\"_0000.nii.gz\", \".nii.gz\")\n",
    "    print(f\"Processing: {img.name}\")\n",
    "\n",
    "    encoder_features.clear()\n",
    "    temp_output = OUT_DIR / f\"temp_{case_id}\"\n",
    "    \n",
    "    try:\n",
    "        predictor.predict_from_files(\n",
    "            [[str(img)]], \n",
    "            [str(temp_output)],\n",
    "            save_probabilities=False,\n",
    "            overwrite=True,\n",
    "            num_processes_preprocessing=1,\n",
    "            num_processes_segmentation_export=1\n",
    "        )\n",
    "        \n",
    "        if temp_output.exists():\n",
    "            temp_output.unlink()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {img.name}: {e}\")\n",
    "        pred = 0\n",
    "        rows.append((case_id, pred))\n",
    "        continue\n",
    "\n",
    "    # Classification with FP16/FP32 fix\n",
    "    if len(encoder_features) >= 3:\n",
    "        try:\n",
    "            valid_features = [f for f in encoder_features if f is not None]\n",
    "            \n",
    "            if len(valid_features) >= 3:\n",
    "                # 🔧 FIX: Convert features to float32 to match classifier weights\n",
    "                valid_features_float = [f.float() for f in valid_features]\n",
    "                logits = classifier(valid_features_float)\n",
    "                pred = int(torch.argmax(logits, dim=1).item())\n",
    "            else:\n",
    "                print(f\"⚠️  Insufficient features captured for {img.name}\")\n",
    "                pred = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Classification error for {img.name}: {e}\")\n",
    "            pred = 0\n",
    "    else:\n",
    "        print(f\"⚠️  No encoder features captured for {img.name}\")\n",
    "        pred = 0\n",
    "\n",
    "    rows.append((case_id, pred))\n",
    "    print(f\"  ✅ Classification: {pred}\")\n",
    "\n",
    "# Cleanup\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# ---- Save Results ----\n",
    "with open(CSV_OUT, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(f\"\\n✅ Test Classification CSV written: {CSV_OUT}\")\n",
    "\n",
    "# Show distribution\n",
    "import pandas as pd\n",
    "df = pd.read_csv(CSV_OUT)\n",
    "print(f\"\\nTest Classification Results:\")\n",
    "print(f\"   Total cases: {len(df)}\")\n",
    "print(f\"   Distribution:\")\n",
    "print(df['Subtype'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnunet_new)",
   "language": "python",
   "name": "nnunet_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
