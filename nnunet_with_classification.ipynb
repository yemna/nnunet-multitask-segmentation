{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282a088-298f-4d8b-ba24-5789aa440bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nnunetv2\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"nnU-Net v2 imported successfully!\")\n",
    "\n",
    "# Test that you can access nnunetv2 modules\n",
    "print(\"nnunetv2 location:\", nnunetv2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaeca7-0cf7-4026-9ccd-24e5e552a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nnunetv2, torch\n",
    "\n",
    "# Work directories - updated for your local setup\n",
    "BASE = \"D:/nnunet_with_classification/data\"\n",
    "RAW  = f\"{BASE}/nnUNet_raw\"\n",
    "PREP = f\"{BASE}/nnUNet_preprocessed\"\n",
    "RES  = f\"{BASE}/nnUNet_results\"\n",
    "\n",
    "# Create nnU-Net directories\n",
    "os.makedirs(RAW, exist_ok=True)\n",
    "os.makedirs(PREP, exist_ok=True)\n",
    "os.makedirs(RES, exist_ok=True)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"nnUNet_raw\"] = RAW\n",
    "os.environ[\"nnUNet_preprocessed\"] = PREP\n",
    "os.environ[\"nnUNet_results\"] = RES\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "print(\"nnU-Net v2: imported successfully!\")\n",
    "print(f\"nnU-Net directories created at: {BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3405a90-353b-41cf-99f6-4659c37083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pathlib, shutil, re, json, csv\n",
    "\n",
    "# >>>>>>>>>>>> UPDATE THIS PATH <<<<<<<<<<<<\n",
    "SRC = \"D:/nnunet_with_classification/data\"\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "RAW  = os.environ[\"nnUNet_raw\"]\n",
    "PREP = os.environ[\"nnUNet_preprocessed\"]\n",
    "DSROOT = f\"{RAW}/{DSNAME}\"\n",
    "imgTr = f\"{DSROOT}/imagesTr\"; lblTr = f\"{DSROOT}/labelsTr\"; imgTs = f\"{DSROOT}/imagesTs\"\n",
    "for d in (imgTr, lblTr, imgTs): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def stem(p): return pathlib.Path(p).name.replace(\".nii.gz\",\"\")\n",
    "sub_regex = re.compile(r\"subtype\\s*([012])\", re.IGNORECASE)\n",
    "\n",
    "def find_split(name):\n",
    "    # case-insensitive split folder lookup\n",
    "    for cand in os.listdir(SRC):\n",
    "        if cand.lower() == name:\n",
    "            p = os.path.join(SRC, cand)\n",
    "            if os.path.isdir(p): return p\n",
    "    return None\n",
    "\n",
    "train_dir = find_split(\"train\")\n",
    "val_dir   = find_split(\"validation\")\n",
    "test_dir  = find_split(\"test\")\n",
    "assert train_dir and val_dir and test_dir, f\"Could not find train/validation/test under {SRC}\"\n",
    "\n",
    "def ingest_split(split_dir, cls_map):\n",
    "    imgs = glob.glob(os.path.join(split_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)\n",
    "    used = 0\n",
    "    for img in sorted(imgs):\n",
    "        case = stem(img).replace(\"_0000\",\"\")\n",
    "        msk  = img.replace(\"_0000.nii.gz\",\".nii.gz\")\n",
    "        if not os.path.exists(msk):\n",
    "            continue\n",
    "        shutil.copy(img, f\"{imgTr}/{case}_0000.nii.gz\")\n",
    "        shutil.copy(msk, f\"{lblTr}/{case}.nii.gz\")\n",
    "        used += 1\n",
    "        # infer subtype from folder names (expects 'subtype0/1/2')\n",
    "        sub_idx = None\n",
    "        for part in pathlib.Path(img).parts:\n",
    "            m = sub_regex.search(part)\n",
    "            if m:\n",
    "                sub_idx = int(m.group(1)); break\n",
    "        if sub_idx is not None:\n",
    "            cls_map[case] = sub_idx\n",
    "    return used\n",
    "\n",
    "cls_map = {}\n",
    "n_tr = ingest_split(train_dir, cls_map)\n",
    "n_va = ingest_split(val_dir,   cls_map)\n",
    "\n",
    "# test images\n",
    "for img in sorted(glob.glob(os.path.join(test_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)):\n",
    "    case = stem(img).replace(\"_0000\",\"\")\n",
    "    shutil.copy(img, f\"{imgTs}/{case}_0000.nii.gz\")\n",
    "\n",
    "print(\"imagesTr:\", len(glob.glob(f\"{imgTr}/*_0000.nii.gz\")))\n",
    "print(\"labelsTr:\", len(glob.glob(f\"{lblTr}/*.nii.gz\")))\n",
    "print(\"imagesTs:\", len(glob.glob(f\"{imgTs}/*_0000.nii.gz\")))\n",
    "print(\"Mapped classification labels:\", len(cls_map))\n",
    "\n",
    "# dataset.json\n",
    "dataset_json = {\n",
    "  \"name\": DSNAME,\n",
    "  \"tensorImageSize\": \"3D\",\n",
    "  \"modality\": {\"0\": \"CT\"},\n",
    "  \"labels\": {\"background\": 0, \"pancreas\": 1, \"lesion\": 2},\n",
    "  \"numTraining\": len(glob.glob(f\"{lblTr}/*.nii.gz\")),\n",
    "  \"numTest\": len(glob.glob(f\"{imgTs}/*_0000.nii.gz\")),\n",
    "  \"training\": [{\"image\": f\"./imagesTr/{stem(i)}.nii.gz\",\n",
    "                \"label\": f\"./labelsTr/{stem(i).replace('_0000','')}.nii.gz\"}\n",
    "               for i in sorted(glob.glob(f\"{imgTr}/*_0000.nii.gz\"))],\n",
    "  \"test\": [f\"./imagesTs/{stem(i)}.nii.gz\" for i in sorted(glob.glob(f\"{imgTs}/*_0000.nii.gz\"))]\n",
    "}\n",
    "os.makedirs(DSROOT, exist_ok=True)\n",
    "with open(f\"{DSROOT}/dataset.json\",\"w\") as f: json.dump(dataset_json, f, indent=2)\n",
    "\n",
    "# splits_final.json (use original validation as nnU-Net val)\n",
    "val_cases = {stem(p).replace(\"_0000\",\"\") for p in glob.glob(os.path.join(val_dir, \"**\", \"*_0000.nii.gz\"), recursive=True)}\n",
    "all_cases = sorted([stem(p).replace(\"_0000\",\"\") for p in glob.glob(f\"{imgTr}/*_0000.nii.gz\")])\n",
    "train_cases = [c for c in all_cases if c not in val_cases]\n",
    "spdir = f\"{PREP}/{DSNAME}\"\n",
    "os.makedirs(spdir, exist_ok=True)\n",
    "with open(f\"{spdir}/splits_final.json\",\"w\") as f:\n",
    "    json.dump([{\"train\": train_cases, \"val\": sorted(list(val_cases))}], f, indent=2)\n",
    "\n",
    "# classification_labels.csv\n",
    "csv_path = f\"{spdir}/classification_labels.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    for c in all_cases:\n",
    "        if c in cls_map:\n",
    "            w.writerow([c, cls_map[c]])\n",
    "        else:\n",
    "            # if you see many misses here, your folders may not be named 'subtype0/1/2'\n",
    "            pass\n",
    "\n",
    "print(\"Wrote dataset.json, splits_final.json, classification_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1501568-3b84-4289-9517-7ac312cc9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, os, pathlib\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "RAW   = os.environ[\"nnUNet_raw\"]\n",
    "DSROOT = f\"{RAW}/{DSNAME}\"\n",
    "lblTr = f\"{DSROOT}/labelsTr\"\n",
    "\n",
    "dataset_v2 = {\n",
    "    \"channel_names\": { \"0\": \"CT\" },               # required in v2\n",
    "    \"labels\": { \"background\": 0, \"pancreas\": 1, \"lesion\": 2 },  # your classes\n",
    "    \"numTraining\": len(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "with open(f\"{DSROOT}/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset_v2, f, indent=2)\n",
    "\n",
    "print(\"âœ… Wrote v2 dataset.json at:\", f\"{DSROOT}/dataset.json\")\n",
    "print(\"numTraining:\", dataset_v2[\"numTraining\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120217f3-ddd3-417f-b7fd-67c08ebebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib, numpy as np, glob, os, pathlib\n",
    "\n",
    "DSID = 777\n",
    "DSNAME = f\"Dataset{DSID:03d}_M31Quiz\"\n",
    "lblTr = os.path.join(os.environ[\"nnUNet_raw\"], DSNAME, \"labelsTr\")\n",
    "\n",
    "bad = []\n",
    "for p in sorted(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))):\n",
    "    img = nib.load(p)\n",
    "    arr = img.get_fdata()  # floats possible\n",
    "    uniq = np.unique(arr)\n",
    "    if not np.all(np.isin(uniq, [0,1,2])):\n",
    "        bad.append((p, uniq))\n",
    "\n",
    "print(\"Masks needing fix:\", len(bad))\n",
    "for p, uniq in bad[:10]:\n",
    "    print(\"  \", pathlib.Path(p).name, \"unique:\", uniq[:10])\n",
    "\n",
    "# Fix: round and clip to {0,1,2}, write as uint8\n",
    "for p, uniq in bad:\n",
    "    img = nib.load(p)\n",
    "    arr = img.get_fdata()\n",
    "    arr = np.rint(arr)           # round to nearest integer\n",
    "    arr = np.clip(arr, 0, 2)     # enforce label set\n",
    "    arr = arr.astype(np.uint8)\n",
    "\n",
    "    hdr = img.header.copy()\n",
    "    hdr.set_data_dtype(np.uint8)\n",
    "    # avoid unintended scaling\n",
    "    hdr[\"scl_slope\"] = 1\n",
    "    hdr[\"scl_inter\"] = 0\n",
    "\n",
    "    fixed = nib.Nifti1Image(arr, img.affine, hdr)\n",
    "    nib.save(fixed, p)\n",
    "\n",
    "# Sanity check after fix\n",
    "remaining = []\n",
    "for p in sorted(glob.glob(os.path.join(lblTr, \"*.nii.gz\"))):\n",
    "    arr = nib.load(p).get_fdata()\n",
    "    uniq = np.unique(arr)\n",
    "    if not np.all(np.isin(uniq, [0,1,2])):\n",
    "        remaining.append((p, uniq))\n",
    "print(\"Remaining masks with non-{0,1,2} labels:\", len(remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b5c41-bdc0-4955-9ec2-026fd80a8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNetv2_plan_and_preprocess -d 777 -c 3d_fullres --verify_dataset_integrity -pl nnUNetPlannerResEncM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09930477-4b85-4ae5-ac1a-02fa99b0dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNetv2_train 777 3d_fullres 0 -tr nnUNetTrainerWithClassification -p nnUNetResEncUNetMPlans --c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e63e24-e6f3-49ad-a97a-4443af3b0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os ,glob\n",
    "ckpt = r\"D:/nnunet_with_classification/data/nnUNet_results/Dataset777_M31Quiz/nnUNetTrainerWithClassification__nnUNetResEncUNetMPlans__3d_fullres/fold_0/checkpoint_best_combined.pth\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "state = sd.get(\"network_state_dict\") or sd.get(\"network_weights\") or sd  # fallbacks\n",
    "hits = [k for k in state.keys() if \"ClassificationHead\" in k]\n",
    "print(\"num ClassificationHead params:\", len(hits))\n",
    "print(hits[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaeed1-b31a-4844-9dee-1f43d886c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/inference.py \\\n",
    "  --model_dir \"D:/nnunet_with_classification/data/nnUNet_results/Dataset777_M31Quiz/nnUNetTrainerWithClassification__nnUNetResEncUNetMPlans__3d_fullres\" \\\n",
    "  --input_dir \"D:/nnunet_with_classification/data/validation_flat\" \\\n",
    "  --output_dir \"D:/nnunet_with_classification/data/validation_flat/predictions\" \\\n",
    "  --fold 0 \\\n",
    "  --checkpoint checkpoint_best_combined.pth \\\n",
    "  --num_classes 3 \\\n",
    "  --no-tta \\\n",
    "  --preproc_workers 1 \\\n",
    "  --export_workers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30128a3-da4f-433c-a852-0fc3d0abe629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation script for your results on validation data\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Validation cases\n",
    "validation_cases = [\n",
    "    \"quiz_0_168\", \"quiz_0_171\", \"quiz_0_174\", \"quiz_0_184\", \"quiz_0_187\", \n",
    "    \"quiz_0_189\", \"quiz_0_244\", \"quiz_0_253\", \"quiz_0_254\", \"quiz_1_090\",\n",
    "    \"quiz_1_093\", \"quiz_1_094\", \"quiz_1_154\", \"quiz_1_158\", \"quiz_1_164\",\n",
    "    \"quiz_1_166\", \"quiz_1_211\", \"quiz_1_213\", \"quiz_1_221\", \"quiz_1_227\",\n",
    "    \"quiz_1_231\", \"quiz_1_242\", \"quiz_1_331\", \"quiz_1_335\", \"quiz_2_074\",\n",
    "    \"quiz_2_080\", \"quiz_2_084\", \"quiz_2_085\", \"quiz_2_088\", \"quiz_2_089\",\n",
    "    \"quiz_2_098\", \"quiz_2_191\", \"quiz_2_241\", \"quiz_2_364\", \"quiz_2_377\",\n",
    "    \"quiz_2_379\"\n",
    "]\n",
    "\n",
    "# Paths (GT from env, PRED paths updated per your request)\n",
    "gt_seg_dir   = f\"{os.environ['nnUNet_raw']}/Dataset777_M31Quiz/labelsTr\"\n",
    "pred_seg_dir = \"D:/nnunet_with_classification/data/validation_flat/predictions\"  # <-- NEW kept\n",
    "gt_cls_csv   = \"D:/nnunet_with_classification/data/validation_labels.csv\"\n",
    "pred_cls_csv = \"D:/nnunet_with_classification/data/validation_flat/predictions/classification_results.csv\"  # <-- NEW kept\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    total = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2.0 * intersection) / total if total > 0 else 1.0\n",
    "\n",
    "# Check paths\n",
    "print(\"ðŸ” Checking paths...\")\n",
    "print(f\"GT segmentation dir:          {gt_seg_dir}   -> {os.path.exists(gt_seg_dir)}\")\n",
    "print(f\"Predicted segmentation dir:   {pred_seg_dir} -> {os.path.exists(pred_seg_dir)}\")\n",
    "print(f\"GT classification CSV:        {gt_cls_csv}   -> {os.path.exists(gt_cls_csv)}\")\n",
    "print(f\"Predicted classification CSV: {pred_cls_csv} -> {os.path.exists(pred_cls_csv)}\")\n",
    "\n",
    "# Segmentation metrics (exact filename match: case.nii.gz)\n",
    "print(\"\\nðŸ“Š Computing segmentation metrics...\")\n",
    "pancreas_dices, lesion_dices, missing_files = [], [], []\n",
    "\n",
    "for case in validation_cases:\n",
    "    gt_file   = f\"{gt_seg_dir}/{case}.nii.gz\"\n",
    "    pred_file = f\"{pred_seg_dir}/{case}.nii.gz\"\n",
    "    \n",
    "    if os.path.exists(gt_file) and os.path.exists(pred_file):\n",
    "        try:\n",
    "            gt   = nib.load(gt_file).get_fdata()\n",
    "            pred = nib.load(pred_file).get_fdata()\n",
    "            \n",
    "            # Whole pancreas (>0)\n",
    "            gt_pancreas   = (gt  > 0).astype(np.uint8)\n",
    "            pred_pancreas = (pred > 0).astype(np.uint8)\n",
    "            pancreas_dices.append(dice_score(gt_pancreas, pred_pancreas))\n",
    "            \n",
    "            # Lesion (== 2) â€” change if your lesion label differs\n",
    "            gt_lesion   = (gt  == 2).astype(np.uint8)\n",
    "            pred_lesion = (pred == 2).astype(np.uint8)\n",
    "            lesion_dices.append(dice_score(gt_lesion, pred_lesion))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {case}: {e}\")\n",
    "            missing_files.append(case)\n",
    "    else:\n",
    "        missing_files.append(case)\n",
    "\n",
    "if pancreas_dices:\n",
    "    whole_pancreas_dsc = float(np.mean(pancreas_dices))\n",
    "    lesion_dsc         = float(np.mean(lesion_dices))\n",
    "    print(f\"Pancreas DSC (mean): {whole_pancreas_dsc:.4f}\")\n",
    "    print(f\"Lesion   DSC (mean): {lesion_dsc:.4f}\")\n",
    "else:\n",
    "    whole_pancreas_dsc = 0.0\n",
    "    lesion_dsc         = 0.0\n",
    "    print(\"No matched segmentation files found.\")\n",
    "\n",
    "# Classification metrics (exact case key; no stripping)\n",
    "print(\"\\nðŸŽ¯ Computing classification metrics...\")\n",
    "gt_labels = {}\n",
    "if os.path.exists(gt_cls_csv):\n",
    "    with open(gt_cls_csv, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if 'case' not in reader.fieldnames or 'label' not in reader.fieldnames:\n",
    "            raise RuntimeError(f\"Expected columns 'case' and 'label' in {gt_cls_csv}\")\n",
    "        for row in reader:\n",
    "            gt_labels[row['case'].strip()] = int(row['label'])\n",
    "\n",
    "pred_labels = {}\n",
    "if os.path.exists(pred_cls_csv):\n",
    "    with open(pred_cls_csv, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if 'case' not in reader.fieldnames:\n",
    "            raise RuntimeError(f\"Expected column 'case' in {pred_cls_csv}\")\n",
    "        # prefer explicit pred_class; else derive from probs p0..pN\n",
    "        has_pred_class = 'pred_class' in reader.fieldnames\n",
    "        prob_cols = [c for c in reader.fieldnames if c.startswith('p') and c[1:].isdigit()]\n",
    "        for row in reader:\n",
    "            case = row['case'].strip()\n",
    "            if has_pred_class and row.get('pred_class', '') != '':\n",
    "                pred_labels[case] = int(row['pred_class'])\n",
    "            elif prob_cols:\n",
    "                probs = np.array([float(row[p]) for p in prob_cols], dtype=float)\n",
    "                pred_labels[case] = int(np.argmax(probs))\n",
    "\n",
    "# Evaluate only overlapping cases (exact name match)\n",
    "val_gt, val_pred = [], []\n",
    "for case in validation_cases:\n",
    "    if case in gt_labels and case in pred_labels:\n",
    "        val_gt.append(gt_labels[case])\n",
    "        val_pred.append(pred_labels[case])\n",
    "\n",
    "if val_gt and val_pred:\n",
    "    macro_f1 = f1_score(val_gt, val_pred, average='macro')\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "else:\n",
    "    macro_f1 = 0.0\n",
    "    print(\"No overlapping classification cases found between GT and predictions.\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n===== SUMMARY =====\")\n",
    "print(f\"Whole Pancreas DSC: {whole_pancreas_dsc:.4f}\")\n",
    "print(f\"Lesion DSC:         {lesion_dsc:.4f}\")\n",
    "print(f\"Macro F1:           {macro_f1:.4f}\")\n",
    "if missing_files:\n",
    "    print(f\"Missing seg files for {len(missing_files)} cases (first few): {missing_files[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac61920-7518-441c-aca0-09d5936bf128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnunet_new)",
   "language": "python",
   "name": "nnunet_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
